# @package training
learning_rate: 3e-3
batch_size: 64
max_epochs: 500
weight_decay: 1e-5
warmup_steps: 300
gradient_clip_val: 1.0
optimizer: "adamw"  # "adamw" or "muon"
loss: "cross_entropy"  # "cross_entropy" or "prob_weighting"