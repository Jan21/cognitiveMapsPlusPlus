# @package training
learning_rate: 0.0025
batch_size: 512
max_epochs: 500
max_steps: -1  # Set to positive number to use steps instead of epochs (-1 = use max_epochs)
weight_decay: 1e-5
warmup_steps: 300
gradient_clip_val: 1.0
optimizer: "adamw"  # "adamw" or "muon"
loss: "cross_entropy"  # "cross_entropy" or "prob_weighting"